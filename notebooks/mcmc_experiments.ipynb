{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7bc27-3d7d-4df4-bea5-8d7880f278ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554f2de-7dc8-44ab-a0f1-8ab8354493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle as pkl\n",
    "import hist\n",
    "from analyzer.datasets import SampleManager\n",
    "from analyzer.core import AnalysisResult\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from analyzer.plotting.plots_1d import drawAs1DHist\n",
    "from analyzer.plotting.plots_2d import drawAs2DHist, addTitles2D\n",
    "from analyzer.plotting.plottables import PlotObject\n",
    "from analyzer.plotting.mplstyles import loadStyles\n",
    "import fitting.regression as regression\n",
    "import fitting.plot_tools as fplt\n",
    "import fitting.high_level as fhl\n",
    "from gpytorch.kernels import ScaleKernel as SK\n",
    "from gpytorch.kernels import RBFKernel as RBF\n",
    "import linear_operator\n",
    "import fitting.models as models\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.close(\"auto\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b48b13-5d52-46b0-9760-c6a334150798",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = AnalysisResult.fromFile(\"../results/data_control.pkl\")\n",
    "\n",
    "sample_manager = SampleManager()\n",
    "sample_manager.loadSamplesFromDirectory(\"../datasets\")\n",
    "hists = res.getMergedHistograms(sample_manager)\n",
    "sig_res = AnalysisResult.fromFile(\"../results/everything.pkl\")\n",
    "sig_res.results[\"signal_312_1500_600\"].histograms[\"h_njet\"] \n",
    "sighists = sig_res.getMergedHistograms(sample_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ead402-abf4-421e-bf60-081ebff6ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_name=\"CR0b_Data2018\"\n",
    "complete_hist = hists[\"ratio_m14_vs_m24\"]\n",
    "narrowed = complete_hist\n",
    "narrowed =  complete_hist[...,hist.loc(1000):hist.loc(3000),hist.loc(0.35):hist.loc(1.0)]\n",
    "narrowed = narrowed[...,::hist.rebin(1),::hist.rebin(1)]\n",
    "qcd_hist = narrowed[bkg_name,...]\n",
    "sig_hist = sighists[\"ratio_m14_vs_m24\"][\"signal_312_1500_900\",hist.loc(1000):hist.loc(3000),hist.loc(0.35):hist.loc(1)]\n",
    "qcd_hist = narrowed[bkg_name,...] \n",
    "true_sum = qcd_hist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efeac79-b579-40c7-964c-24a5c00b250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax= plt.subplots(1,2,figsize=(10,3))\n",
    "_ = drawAs2DHist(ax[0], PlotObject.fromHist(qcd_hist))\n",
    "_ = drawAs2DHist(ax[1], PlotObject.fromHist(sig_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c599108-ca84-400f-a6fb-a2db1df7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitting.regression as fr\n",
    "window = [(1300, 1750), (400, 700)]\n",
    "rebin = 1\n",
    "mask_f = fr.rectMasker(window)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1500.0,600.0]), 200.0,150.0)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1400.0,0.6]), 150.0,0.05)\n",
    "mask_f = fr.ellipseMasker(torch.tensor([2000.0,0.6]), 150.0,0.05)\n",
    "mask_f = fr.ellipseMasker(torch.tensor([1200.0,0.5]), 100.0,0.05)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1800.0,0.6]), 50.0,0.02)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1500.0,1400.0]), 200.0,150.0)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1450.0,0.59]), 150.0,0.05)\n",
    "#mask_f=None\n",
    "\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1200.0,0.6]), 200.0,0.05)\n",
    "exclude_less=0.001\n",
    "#mask_f = None\n",
    "train_data = regression.makeRegressionData(qcd_hist[hist.rebin(rebin),hist.rebin(rebin)], mask_f, exclude_less=exclude_less)\n",
    "test_data, m = regression.makeRegressionData(qcd_hist[hist.rebin(rebin),hist.rebin(rebin)], None, exclude_less=exclude_less, get_mask=True)\n",
    "train_transform = regression.getNormalizationTransform(train_data)\n",
    "normalized_train_data = train_transform.transform(train_data)\n",
    "normalized_test_data = train_transform.transform(test_data)\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(10,10),layout=\"tight\")\n",
    "fplt.simpleGrid(ax[0][0], train_data.E, train_data.X, train_data.Y)\n",
    "mask = regression.getBlindedMask(test_data.X, test_data.Y, test_data.Y, test_data.V, mask_f)\n",
    "squares = fplt.makeSquares(test_data.X[mask], test_data.E)\n",
    "points = fplt.getPolyFromSquares(squares)\n",
    "_=drawAs2DHist(ax[0][1], PlotObject.fromHist(sig_hist))\n",
    "poly = matplotlib.patches.Polygon(points, edgecolor=\"red\", fill=False)\n",
    "ax[0][1].add_patch(poly)\n",
    "signal_data = regression.makeRegressionData(sig_hist[hist.rebin(rebin),hist.rebin(rebin)])\n",
    "signal_data = regression.DataValues(signal_data.X[m], signal_data.Y[m], signal_data.V[m], signal_data.E)\n",
    "fplt.simpleGrid(ax[1][0], signal_data.E, signal_data.X, signal_data.Y)\n",
    "fplt.simpleGrid(ax[1][1], normalized_train_data.E, normalized_train_data.X, normalized_train_data.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1186a7-fa73-4909-b57a-be8f4cb2c932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = True \n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=regression.sendToGpu(normalized_train_data)\n",
    "    #train=regression.sendToGpu(train_data)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "linear=gpytorch.kernels.LinearKernel(ard_num_dims=2,num_dimensions=2)\n",
    "rq=SK(gpytorch.kernels.RQKernel(ard_num_dims=2))\n",
    "\n",
    "smk = gpytorch.kernels.SpectralMixtureKernel(ard_num_dims=2, num_mixtures=8)\n",
    "gsmk=models.GeneralSpectralMixture(num_mixtures=1, ard_num_dims=2)\n",
    "#peak_smk.initialize_from_data(train.X, train.Y)\n",
    "\n",
    "peaked_rbf=SK(models.PeakedRBF(ard_num_dims=2))\n",
    "peaked_grbf=SK(models.PeakedGRBF(ard_num_dims=2))\n",
    "\n",
    "NNRBF = models.wrapNN(\"NNRBFKernel\", gpytorch.kernels.RBFKernel)\n",
    "NNRQ = models.wrapNN(\"NNRQKernel\", gpytorch.kernels.RQKernel)\n",
    "NNGRBF = models.wrapNN(\"NNRBFKernel\", models.GeneralRBF)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grq=SK(models.GeneralRQ(ard_num_dims=2))\n",
    "grbf = SK(models.GeneralRBF(ard_num_dims=2))\n",
    "gmatern = SK(models.GeneralMatern(ard_num_dims=2))\n",
    "poly = SK(gpytorch.kernels.PiecewisePolynomialKernel(ard_num_dims=2))\n",
    "lk = SK(gpytorch.kernels.LinearKernel(ard_num_dims=2,num_dimensions=2))\n",
    "rbf = SK(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "matern = SK(gpytorch.kernels.MaternKernel(ard_num_dims=2))\n",
    "\n",
    "\n",
    "NNSMKernel = models.wrapNN(\"NNSMKernel\", gpytorch.kernels.SpectralMixtureKernel)\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=(256,64,32,16,4,)))\n",
    "#nnrbf = SK(NNRBF(odim=2,layer_sizes=(32,32,16)))\n",
    "#nnrbf = SK(NNRBF(odim=2,layer_sizes=(256,128,64,32)))\n",
    "nnsmk = NNSMKernel(idim=2, odim=2, num_mixtures=2)\n",
    "\n",
    "\n",
    "#shape = (256, 64,32)\n",
    "#shape = (256,128,16)\n",
    "#shape = (1024,1024,128)\n",
    "shape = (32,16,8)\n",
    "\n",
    "\n",
    "\n",
    "nngrbf = SK(NNGRBF(odim=2,layer_sizes=shape))\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=shape))\n",
    "\n",
    "if use_cuda and False:\n",
    "    smk = smk.cuda()\n",
    "    nnsmk = nnsmk.cuda()\n",
    "    smk.initialize_from_data(train.X, train.Y)\n",
    "    nnsmk.initialize_from_data(nnsmk.feature_extractor(train.X),train.Y)\n",
    "\n",
    "#model,likelihood = regression.createModel(train, kernel=grbf)\n",
    "\n",
    "grid_size = gpytorch.utils.grid.choose_grid_size(train.X,1.0)\n",
    "grid_size = 10\n",
    "print(grid_size)\n",
    "grid_rbf = SK(gpytorch.kernels.GridInterpolationKernel(\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=2),grid_size=grid_size,num_dims=2))\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train.V, learn_additional_noise=False)\n",
    "model=models.InducingPointModel(train.X,train.Y, \n",
    "                                likelihood, \n",
    "                                kernel=grbf, \n",
    "                                inducing=train.X[::2])\n",
    "#model,likelihood = regression.createModel(train, kernel=grbf)\n",
    "if torch.cuda.is_available() and use_cuda:       \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    print(\"Loaded model to gpu\")\n",
    "print(model)\n",
    "\n",
    "#model = model.cuda()\n",
    "def pyro_model(x, y):\n",
    "    with gpytorch.settings.fast_computations(False, False, False):\n",
    "        sampled_model = model.pyro_sample_from_prior()\n",
    "        output = sampled_model.likelihood(sampled_model(x))\n",
    "        print(output.mean)\n",
    "        print(y)\n",
    "        pyro.sample(\"obs\", output, obs=y)\n",
    "    return y\n",
    "#print(train.X)\n",
    "#nuts_kernel = NUTS(pyro_model, adapt_step_size=True)\n",
    "#mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=10)\n",
    "#mcmc_run.run(train.X, train.Y)\n",
    "\n",
    "model,likelihood = regression.optimizeHyperparams(model,likelihood, train, bar=False, iterations=200, lr=0.1)\n",
    "print(\"DONE\")\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model from gpu\")\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()\n",
    "    print(\"Loaded model from gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd68c8e-884e-4dc4-a6a4-bd1d12d7f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if mask_f:\n",
    "  #  mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "\n",
    "p = regression.getPrediction(model, likelihood, normalized_test_data)\n",
    "e = linear_operator.utils.cholesky.psd_safe_cholesky(p.covariance_matrix)\n",
    "cm = e @ e.T\n",
    "pred_dist=p\n",
    "print(torch.allclose(cm,p.covariance_matrix))\n",
    "#pred_dist = gpytorch.distributions.MultivariateNormal(p.mean,cm)\n",
    "#pred_dist = likelihood(pred_dist)\n",
    "pred = train_transform.iTransform(regression.DataValues(normalized_test_data.X, pred_dist.mean, pred_dist.variance.detach(), normalized_test_data.E))\n",
    "\n",
    "if mask_f:\n",
    "    mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "    bpred_mean = pred.Y[mask]\n",
    "    obs_mean = test_data.Y[mask]\n",
    "    obs_var = test_data.V[mask]\n",
    "    chi2 = torch.sum((obs_mean - bpred_mean)**2 / obs_var) / torch.count_nonzero(mask)\n",
    "    avg_pull = torch.sum(torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)) / torch.count_nonzero(mask)\n",
    "    print(f\"Chi^2/bins = {chi2}\")\n",
    "    print(f\"Avg Abs pull = {avg_pull}\")\n",
    "else:\n",
    "    mask=None\n",
    "if avg_pull < 5:    \n",
    "    p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280093b-0950-4fbc-90f8-2eb5b38190aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters_and_constraints())\n",
    "for name,param,constraint in params:\n",
    "    if constraint:\n",
    "        real_param = constraint.transform(param)\n",
    "    else:\n",
    "        real_param = param\n",
    "    print(f\"{name.replace('raw_',''):{max(len(x[0]) for x in params)+4}} {real_param.detach().numpy().round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dba932-f0e4-4f3b-80d7-2b4630a0c7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a040f-57f0-473a-9f83-a73e124dc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "e,e2=jnp.linalg.eigh(cmnp)\n",
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b30dd-f44d-4b90-9a90-62e3d3c5eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import jax.numpy as jnp\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.handlers import condition, seed, substitute, trace\n",
    "\n",
    "\n",
    "V = linear_operator.utils.cholesky.psd_safe_cholesky(pred_dist.covariance_matrix).numpy()\n",
    "cm = V @ V.T\n",
    "evals,evecs = jnp.linalg.eigh(cm)\n",
    "evals = evals[::-1]\n",
    "evecs = evecs[::-1]\n",
    "eva = evals[:50]\n",
    "eve = evecs[:50].T\n",
    "\n",
    "evam = jnp.sqrt(jnp.diag(eva))\n",
    "#print(jnp.diag(eva).shape)\n",
    "#print(eve.shape)\n",
    "X= jnp.matmul(eve, evam)\n",
    "print(V.shape)\n",
    "print(V)\n",
    "\n",
    "mu = pred_dist.mean.numpy()\n",
    "sd = signal_data.Y.numpy()\n",
    "print(sd)\n",
    "\n",
    "def pyro_model(observation=None):        \n",
    "    r= numpyro.sample(\"rate\",dist.Uniform(0,20))  \n",
    "    #with numpyro.plate(\"background_variations\", eva.shape[0]): \n",
    "    #    background = numpyro.sample(\"background_estimation_raw\", dist.Normal(0,1))\n",
    "    with numpyro.plate(\"background_variations\", V.shape[0]): \n",
    "        b = numpyro.sample(\"background_estimation_raw\", dist.Normal(0,1))\n",
    "    background = mu + V @ b        \n",
    "    obs_hist = (r * sd) +  transform.iTransformData(background)\n",
    "    with numpyro.plate(\"bins\", mu.shape[0]):         \n",
    "        return numpyro.sample(\"observed\", dist.Normal(jnp.clip(obs_hist,1),jnp.sqrt(jnp.clip(obs_hist,1))), obs=observation) \n",
    "        #return numpyro.sample(\"observed\", dist.Poisson(jnp.ceil(jnp.clip(obs_hist,1))), obs=observation)\n",
    "\n",
    "observation = transform.iTransformData(mu) + 10 * sd\n",
    "observation = jnp.ceil(jnp.clip(obs,1))\n",
    "\n",
    "#conditioned = condition(pyro_model, {\"observed\": obs})\n",
    "#tm = reparam(pyro_model, config={\"background_estimation\": LocScaleReparam(0)})\n",
    "print(obs)\n",
    "if False:\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.hist([pyro_model(observation=observation)[1].cpu() for x in range(1000)])\n",
    "\n",
    "conditioned = condition(pyro_model, {\"observed\" : observation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c611b-fc5c-40d9-8217-1e1dace2539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(0)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "nuts_kernel = NUTS(conditioned,adapt_step_size=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, num_warmup=500,num_chains=1)\n",
    "mcmc.run(rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c08b6-d782-4456-8278-18fc9fadea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0540a0b-4aa4-492c-ad6a-08c8c1c0bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = torch.median(points, axis=0).values\n",
    "    print(median)\n",
    "    diff = torch.sum((points - median)**2, axis=-1)\n",
    "    diff = torch.sqrt(diff)\n",
    "    med_abs_deviation = torch.median(diff)\n",
    "    print(med_abs_deviation)\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "    return modified_z_score > thresh\n",
    "def noOutliers(a, thresh = 3.5):\n",
    "    return a[~is_outlier(a,thresh)]\n",
    "    \n",
    "s = mcmc.get_samples()\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.hist(s['rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6897e6-6258-4385-be1a-74a15b2cdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "guide = pyro.infer.autoguide.AutoNormal(conditioned)\n",
    "#def guide():\n",
    "#    pass\n",
    "num_steps = 4000\n",
    "initial_lr = 0.1\n",
    "gamma = 0.01  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "adam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "#adam = pyro.optim.Adam({\"lr\": 0.01})  # Consider decreasing learning rate.\n",
    "elbo = pyro.infer.Trace_ELBO()\n",
    "svi = pyro.infer.SVI(conditioned, guide, adam, elbo)\n",
    "losses = []\n",
    "for step in range(num_steps):  # Consider running for more steps.\n",
    "    loss = svi.step()\n",
    "    losses.append(loss)\n",
    "    if step % ( num_steps // 10) == 0:\n",
    "        #print(f\"r = {pyro.param('r').item()}\")\n",
    "        print(\"Elbo loss: {:0.3f}\".format(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d715b90-5792-4be9-82ce-3d4c3781eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "predictive = Predictive(conditioned, guide=guide, num_samples=1000)\n",
    "p = predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ec4d5-1c1e-48c8-ab54-b1153855e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = p['rate'].detach().flatten()\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(r,bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad459a-c0e9-4e98-879a-5ac5fca67f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13ce1f-ab59-4920-9320-edcb9b50aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitting.plot_tools as fpt\n",
    "import analyzer.plotting as plotting\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "importlib.reload(fpt)\n",
    "dim = 1\n",
    "\n",
    "pred_mean,_ = fitting.regression.pointsToGrid(test_data.X, pred.Y, test_data.E)\n",
    "pred_var,_ = fitting.regression.pointsToGrid(test_data.X, pred.V, test_data.E)\n",
    "obs_vals,_ = fitting.regression.pointsToGrid(test_data.X, test_data.Y, test_data.E)\n",
    "obs_vars, filled = fitting.regression.pointsToGrid(test_data.X, test_data.V, test_data.E)\n",
    "\n",
    "#(figpath /\"slices\" / f\"along_{dim}\").mkdir(parents=True, exist_ok=True)\n",
    "for val, f, ax in fpt.createSlices(\n",
    "    pred_mean.hist,\n",
    "    pred_var.hist,\n",
    "    obs_vals.hist,\n",
    "    obs_vars.hist,\n",
    "    test_data.E,\n",
    "    filled,\n",
    "    mask_function=mask_f,\n",
    "    observed_title=\"CRData\", slice_dim=dim, just_window=True):\n",
    "    plotting.addTitles1D(ax, plotting.PlotObject.fromHist(qcd_hist[:,sum]))\n",
    "    #f.savefig(figpath /\"slices\" / f\"along_{dim}\" /  (f\"slice_{round(float(val),3)}\".replace(\".\",\"p\") + \".pdf\"))\n",
    "    #plt.close(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b85528-bb0a-434b-a448-f50ddf5c8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ApproximateGP, GP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=regression.sendToGpu(normalized_train_data)\n",
    "    test=regression.sendToGpu(normalized_test_data)\n",
    "    #train = regression.DataValues(normalized_train_data.X.cuda(), normalized_train_data.Y.cuda(), normalized_train_data.V.cuda(), None)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "    test = normalized_train_data\n",
    "\n",
    "smoke_test = True\n",
    "train_x, train_y, train_v = train.X, train.Y, train.V\n",
    "test_x, test_y, test_v = test.X, test.Y, test.V\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y, train_v)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100000, shuffle=True)\n",
    "\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant'):\n",
    "        if output_dims is None:\n",
    "            inducing_points = torch.randn(num_inducing, input_dims) + 1\n",
    "            batch_shape = torch.Size([])\n",
    "        else:\n",
    "            inducing_points = torch.randn(output_dims, num_inducing, input_dims) + 1\n",
    "            batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        if mean_type == 'constant':\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        else:\n",
    "            self.mean_module = LinearMean(input_dims)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape, ard_num_dims=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, *other_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily. For example, hidden_layer2(hidden_layer1_outputs, inputs) will pass the concatenation of the first\n",
    "        hidden layer's outputs and the input data to hidden_layer2.\n",
    "        \"\"\"\n",
    "        if len(other_inputs):\n",
    "            if isinstance(x, gpytorch.distributions.MultitaskMultivariateNormal):\n",
    "                x = x.rsample()\n",
    "\n",
    "            processed_inputs = [\n",
    "                inp.unsqueeze(0).expand(gpytorch.settings.num_likelihood_samples.value(), *inp.shape)\n",
    "                for inp in other_inputs\n",
    "            ]\n",
    "\n",
    "            x = torch.cat([x] + processed_inputs, dim=-1)\n",
    "        return super().__call__(x, are_samples=bool(len(other_inputs)))\n",
    "\n",
    "num_hidden_dims =  10\n",
    "\n",
    "class DeepGP(DeepGP):\n",
    "    def __init__(self, train_x_shape, train_vars):\n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=num_hidden_dims,\n",
    "            mean_type='linear',\n",
    "        )\n",
    "        \n",
    "        middle_hidden = ToyDeepGPHiddenLayer(\n",
    "            input_dims=num_hidden_dims,\n",
    "            output_dims=num_hidden_dims,\n",
    "            mean_type='linear',\n",
    "        )\n",
    "\n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=middle_hidden.output_dims,\n",
    "            output_dims=None,\n",
    "            mean_type='constant',\n",
    "        )\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.last_layer = last_layer\n",
    "        self.middle_hidden = middle_hidden\n",
    "        self.likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train_vars)\n",
    "        #self.likelihood = gpytorch.likelihoods.GaussianLi(noise=train_vars)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer(inputs)\n",
    "        x = self.middle_hidden(hidden_rep1)\n",
    "        output = self.last_layer(hidden_rep1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            mus = []\n",
    "            variances = []\n",
    "            lls = []\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                preds = self.likelihood(self(x_batch))\n",
    "                mus.append(preds.mean)\n",
    "                variances.append(preds.variance)\n",
    "                lls.append(model.likelihood.log_marginal(y_batch, model(x_batch)))\n",
    "        \n",
    "        return torch.cat(mus, dim=-1).mean(0), torch.cat(variances, dim=-1).mean(0), torch.cat(lls, dim=-1)\n",
    "\n",
    "model = DeepGP(train_x.shape,train_v)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "num_epochs =  5000\n",
    "num_samples = 4\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},], lr=0.05)\n",
    "mll = DeepApproximateMLL(VariationalELBO(model.likelihood, model, train_x.shape[-2]))\n",
    "\n",
    "epochs_iter = range(num_epochs)\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data    minibatch_iter = train_loader\n",
    "    \n",
    "    #for x_batch, y_batch, noise in train_loader:\n",
    "    with gpytorch.settings.num_likelihood_samples(num_samples):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y, noise=train_v)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % (num_epochs // 10) == 0 :\n",
    "        print(f\"Epoch {i:3}: Loss {loss:0.3f}\")\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e89c91-b95c-4a4d-8030-541b3e3943dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9a90e-7656-4694-8ece-f3a2206763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000000)\n",
    "\n",
    "model.eval()\n",
    "#predictive_means, predictive_variances, test_lls = model.predict(test_loader)\n",
    "with torch.no_grad(): \n",
    "    p = model.likelihood(model(test_x),noise=test_v)\n",
    "    predictive_means = p.mean.mean(0)\n",
    "    predictive_variances = p.variance.mean(0)\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    predictive_means=predictive_means.cpu()\n",
    "    predictive_variances=predictive_variances.cpu()\n",
    "    #train = regression.DataValues(normalized_train_data.X.cuda(), normalized_train_data.Y.cuda(), normalized_train_data.V.cuda(), None)\n",
    "\n",
    "pred = test_transform.iTransform(\n",
    "    regression.DataValues(normalized_test_data.X, predictive_means, predictive_variances, normalized_test_data.E))\n",
    "\n",
    "import fitting.high_level as fhl\n",
    "predictive_means\n",
    "\n",
    "p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c252f-b0f7-4584-a48e-4b4c0c45de5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349254f-c594-4833-ac07-34a12a3afcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026d8ce-1346-4d4a-be54-e24acca254f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "class PyroGPModel(gpytorch.models.PyroGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, inducing_points, kernel=None, mean=None):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0),\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, \n",
    "            variational_distribution, \n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(\n",
    "            variational_strategy,\n",
    "            likelihood,\n",
    "            num_data=train_y.numel(),\n",
    "            name_prefix=\"simple_regression_model\"\n",
    "        )\n",
    "        self.likelihood = likelihood\n",
    "        self.mean_module = mean or gpytorch.means.ConstantMean()\n",
    "        if kernel is None:\n",
    "            kernel = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=2)\n",
    "            )\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)  # Returns an n_data vec\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points,kernel=None, mean=None):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, \n",
    "            inducing_points, \n",
    "            variational_distribution, \n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "        self.mean_module = mean or gpytorch.means.ConstantMean()\n",
    "        if kernel is None:\n",
    "            kernel = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "use_cuda = True \n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=regression.sendToGpu(normalized_train_data)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "\n",
    "NNRBF = fitting.models.wrapNN(\"NNRBFKernel\", gpytorch.kernels.RBFKernel)\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=(256,128,64,32)))\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=(32,16)))\n",
    "\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train.V)\n",
    "model=StandardApproximateGP(\n",
    "                 #train.X,\n",
    "                 #train.Y, \n",
    "                 #likelihood, \n",
    "                 train.X[::4],\n",
    "                 kernel=grbf\n",
    "              )\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    print(\"Loaded model to gpu\")\n",
    "\n",
    "def trainModel(model, likelihood, lr=0.01, num_iter = 1):\n",
    "    optimizer = pyro.optim.Adam({\"lr\": lr})\n",
    "    elbo = pyro.infer.Trace_ELBO(num_particles=128, vectorize_particles=True, retain_graph=True)\n",
    "    svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        model.zero_grad()\n",
    "        loss = svi.step(train.X, train.Y)\n",
    "        if i % (num_iter // 10) == 0 :\n",
    "            print(f\"Iter {i}: Loss {loss:2}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def trainModel2(model, likelihood,lr=0.1,num_iter = 1):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=lr)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train.Y.size(0))\n",
    "    \n",
    "    \n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train.X)\n",
    "        loss = -mll(output, train.Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % (num_iter // 10) == 0 :\n",
    "            print(f\"Iter {i}: Loss {loss:2}\")\n",
    "    \n",
    "    print(f\"Iter {i}: Loss {loss:2}\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "print(model)\n",
    "model = trainModel2(model,likelihood,lr=0.025,num_iter=10000)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DONE\")\n",
    "#with linear_operator.settings.max_cg_iterations(2000):\n",
    "#    model,likelihood = regression.optimizeHyperparams(model,likelihood, train, bar=False, iterations=250, lr=0.05)\n",
    "print(\"DONE\")\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()\n",
    "    print(\"Loaded model to gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b65fe-b7f6-4c1d-86db-4339535c8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters_and_constraints())\n",
    "for name,param,constraint in params:\n",
    "    if constraint:\n",
    "        real_param = constraint.transform(param)\n",
    "    else:\n",
    "        real_param = param\n",
    "    print(f\"{name.replace('raw_',''):{max(len(x[0]) for x in params)+4}} {real_param.detach().numpy().round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ab348-f4ee-4084-a80c-b60162c7648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_dist = regression.getPrediction(model, likelihood, normalized_test_data)\n",
    "pred = test_transform.iTransform(    regression.DataValues(normalized_test_data.X, pred_dist.mean, pred_dist.variance, normalized_test_data.E))\n",
    "\n",
    "if window:\n",
    "    mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "    bpred_mean = pred.Y[mask]\n",
    "    obs_mean = test_data.Y[mask]\n",
    "    obs_var = test_data.V[mask]\n",
    "    chi2 = torch.sum((obs_mean - bpred_mean)**2 / obs_var) / torch.count_nonzero(mask)\n",
    "    avg_pull = torch.sum(torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)) / torch.count_nonzero(mask)\n",
    "    print(f\"Chi^2/bins = {chi2}\")\n",
    "    print(f\"Avg Abs pull = {avg_pull}\")\n",
    "if avg_pull < 5:    \n",
    "    p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ab9cb-a86a-459d-a851-c80854aae2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)\n",
    "_,ax1 = p[\"gpr_mean\"]\n",
    "indp=model.covar_module.inducing_points\n",
    "ind = test_transform.transform_x.iTransformData(indp).detach()\n",
    "\n",
    "#ax1.scatter(ind[:,0], ind[:,1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsmlenv",
   "language": "python",
   "name": "cmsmlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
