{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7bc27-3d7d-4df4-bea5-8d7880f278ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2bcd8-ec64-496a-a953-679364a8179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
    "\n",
    "    log = file if hasattr(file,'write') else sys.stderr\n",
    "    traceback.print_stack(file=log)\n",
    "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "#warnings.showwarning = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554f2de-7dc8-44ab-a0f1-8ab8354493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle as pkl\n",
    "import hist\n",
    "from analyzer.datasets import SampleManager, ProfileRepo\n",
    "from analyzer.core import AnalysisResult\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from analyzer.plotting.plots_1d import drawAs1DHist\n",
    "from analyzer.plotting.plots_2d import drawAs2DHist, addTitles2D\n",
    "from analyzer.plotting.plottables import PlotObject\n",
    "from analyzer.plotting.mplstyles import loadStyles\n",
    "import fitting.regression as regression\n",
    "import fitting.plot_tools as fplt\n",
    "import fitting.high_level as fhl\n",
    "from gpytorch.kernels import ScaleKernel as SK\n",
    "from gpytorch.kernels import RBFKernel as RBF\n",
    "import linear_operator\n",
    "import fitting.models as models\n",
    "import fitting.utils as futil\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.close(\"auto\")\n",
    "loadStyles()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b48b13-5d52-46b0-9760-c6a334150798",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = AnalysisResult.fromFile(\"../results/histograms/2018mc.pkl\")\n",
    "profile_repo = ProfileRepo()                                                                                                                                                                            \n",
    "profile_repo.loadFromDirectory(\"../profiles\")              \n",
    "sample_manager = SampleManager()\n",
    "sample_manager.loadSamplesFromDirectory(\"../datasets\", profile_repo)\n",
    "\n",
    "\n",
    "hists = res.getMergedHistograms(sample_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ead402-abf4-421e-bf60-081ebff6ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_name=\"QCDInclusive2018\"\n",
    "#bkg_name=\"Skim_QCDInclusive2018\"\n",
    "h = \"ratio_m24_vs_m14\"\n",
    "complete_hist = hists[h]\n",
    "integral_cr = complete_hist[bkg_name].sum()\n",
    "m = (slice(None,None,hist.rebin(1)),slice(None,None,hist.rebin(1)))\n",
    "#narrowed =  complete_hist[bkg_name][...,hist.loc(1000):hist.loc(3000),hist.loc(0.35):hist.loc(1.0)]\n",
    "narrowed = complete_hist[bkg_name][m]\n",
    "\n",
    "sig_hist = hists[h][\"signal_312_1200_400\"][m]\n",
    "\n",
    "qcd_hist = narrowed  \n",
    "true_sum = qcd_hist.sum()\n",
    "print(true_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efeac79-b579-40c7-964c-24a5c00b250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax= plt.subplots(1,2,figsize=(10,3))\n",
    "_ = drawAs2DHist(ax[0], PlotObject.fromHist(qcd_hist))\n",
    "_ = drawAs2DHist(ax[1], PlotObject.fromHist(sig_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c599108-ca84-400f-a6fb-a2db1df7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitting.regression as fr\n",
    "from fitting.windowing import EllipseWindow\n",
    "from fitting.transformations import getNormalizationTransform\n",
    "\n",
    "window = [(1300, 1750), (400, 700)]\n",
    "rebin = 1\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1400.0,0.6]), 200.0, 0.05)\n",
    "mask_f = EllipseWindow([1150.0,0.33], [120.0,0.06])\n",
    "#mask_f = EllipseWindow([1150.0,400.0], [120.0,100])\n",
    "#mask_f = EllipseWindow([1150.0,0.9], [120.0,0.06])\n",
    "\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([2000.0,0.6]), 150.0,0.05)\n",
    "#mask_f = fr.ellipseMasker(torch.ten can always look at another method.sor([1200.0,0.5]), 100.0,0.05)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1800.0,0.6]), 50.0,0.02)\n",
    "\n",
    "\n",
    "\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1500.0,1400.0]), 200.0,150.0)\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1450.0,0.59]), 150.0,0.05)\n",
    "#mask_f=None\n",
    "\n",
    "#mask_f = fr.ellipseMasker(torch.tensor([1200.0,0.6]), 200.0,0.05)\n",
    "exclude_less=0.001\n",
    "#mask_f = None\n",
    "train_data = regression.makeRegressionData(qcd_hist[hist.rebin(rebin),hist.rebin(rebin)], mask_f, exclude_less=exclude_less)\n",
    "test_data, m = regression.makeRegressionData(qcd_hist[hist.rebin(rebin),hist.rebin(rebin)], None, exclude_less=exclude_less, get_mask=True)\n",
    "train_transform = getNormalizationTransform(train_data)\n",
    "normalized_train_data = train_transform.transform(train_data)\n",
    "normalized_test_data = train_transform.transform(test_data)\n",
    "\n",
    "fig,ax = plt.subplots(3,2,figsize=(10,10),layout=\"tight\")\n",
    "fplt.simpleGrid(ax[0][0], train_data.E, train_data.X, train_data.Y)\n",
    "mask = regression.getBlindedMask(test_data.X, test_data.Y, test_data.Y, test_data.V, mask_f)\n",
    "squares = fplt.makeSquares(test_data.X[mask], test_data.E)\n",
    "points = fplt.getPolyFromSquares(squares)\n",
    "_=drawAs2DHist(ax[0][1], PlotObject.fromHist(sig_hist))\n",
    "poly = matplotlib.patches.Polygon(points, edgecolor=\"red\", fill=False)\n",
    "ax[0][1].add_patch(poly)\n",
    "signal_data = regression.makeRegressionData(sig_hist[hist.rebin(rebin),hist.rebin(rebin)])\n",
    "signal_data = regression.DataValues(signal_data.X[m], signal_data.Y[m], signal_data.V[m], signal_data.E)\n",
    "fplt.simpleGrid(ax[1][0], signal_data.E, signal_data.X, signal_data.Y)\n",
    "fplt.simpleGrid(ax[1][1], normalized_train_data.E, normalized_train_data.X, normalized_train_data.Y)\n",
    "fplt.simpleGrid(ax[2][1], test_data.E, test_data.X, test_data.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f667d-bfdd-41f0-953f-d5100d6cba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_train_data.Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dc71f-64f7-4975-b24b-6911b66b4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.densenet import DenseNet\n",
    "\n",
    "class DenseNetFeatureExtractor(DenseNet):\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)\n",
    "        return out\n",
    "\n",
    "feature_extractor = DenseNetFeatureExtractor(block_config=(6, 6, 6), num_classes=2)\n",
    "num_features = feature_extractor.classifier.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef06d0a-1fb0-4eb4-bf87-86fd18473763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inducing_points = inducing_points.cuda()\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=d)\n",
    "\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a98514-c5c6-4321-b54a-526082e6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "data_dim = test_data.X.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 32))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(32, 128))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linearx', torch.nn.Linear(128, 64))\n",
    "        self.add_module('relux', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(64, 32))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(32, 16))\n",
    "        self.add_module('relu4', torch.nn.ReLU())\n",
    "        self.add_module('linear5', torch.nn.Linear(16, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d6000-d60b-4031-9293-bdbf513c0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.9041,  0.96], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
    "b = torch.tensor([[0.9,0.9]], dtype=torch.float64)\n",
    "torch.squeeze(torch.cdist(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a3c66-ce83-4a05-b0c9-e2a21ec5c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.constraints import Positive\n",
    "class MultipliedRBF(models.GeneralRBF):\n",
    "    is_stationary = False\n",
    "\n",
    "    def f(self, x1):\n",
    "        C = torch.squeeze(torch.cdist(x1,self.point),dim=1)\n",
    "        return C\n",
    "    def __init__(self, *args,point_prior=None, point_constraint=None, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "        self.register_parameter(\n",
    "            name='raw_point', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1,2))\n",
    "        )\n",
    "\n",
    "        # set the parameter constraint to be positive, when nothing is specified\n",
    "        if point_constraint is None:\n",
    "            point_constraint = Positive()\n",
    "\n",
    "        # register the constraint\n",
    "        self.register_constraint(\"raw_point\", point_constraint)\n",
    "        # set the parameter prior, see\n",
    "        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n",
    "        if point_prior is not None:\n",
    "            self.register_prior(\n",
    "                \"point_prior\",\n",
    "                point_prior,\n",
    "                lambda m: m.point,\n",
    "                lambda m, v : m._set_point(v),\n",
    "            )\n",
    "\n",
    "    # now set up the 'actual' paramter\n",
    "    @property\n",
    "    def point(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        return self.raw_point_constraint.transform(self.raw_point)\n",
    "\n",
    "    @point.setter\n",
    "    def point(self, value):\n",
    "        return self._set_point(value)\n",
    "\n",
    "    def _set_length(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_point)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_point=self.raw_point_constraint.inverse_transform(value))\n",
    "\n",
    "        \n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        fx1 = self.f(x1)\n",
    "        fx2 = self.f(x2)\n",
    "        c = super().forward(x1,x2,**params)\n",
    "        outer = torch.outer(fx1,fx2)\n",
    "        return c * outer\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "           \n",
    "            self.covar_module = ( #gpytorch.kernels.InducingPointKernel(\n",
    "                gpytorch.kernels.ScaleKernel(\n",
    "                models.GeneralRBF(ard_num_dims=2) * \n",
    "                       MultipliedRBF(ard_num_dims=2) \n",
    "\n",
    "                ),\n",
    "            #    inducing_points=train.X[::].clone(), likelihood=likelihood\n",
    "           # )\n",
    "            )\n",
    "            #self.feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            #self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            #projected_x = self.feature_extractor(x)\n",
    "            #projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "            projected_x = x\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b061fa-81fa-40b1-a479-db8af5d73a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVGPRegressionModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        # Define all the variational stuff\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=train_y.numel() ,\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, train_x, variational_distribution\n",
    "        )\n",
    "\n",
    "        # Standard initializtation\n",
    "        super(PVGPRegressionModel, self).__init__(\n",
    "            variational_strategy,\n",
    "          #  likelihood,\n",
    "           # num_data=train_y.numel(),\n",
    "              #  name_prefix=\"simple_regression_model\"\n",
    "        )\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "        # Mean, covar\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            models.GeneralRBF( ard_num_dims=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)  # Returns an n_data vec\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa4bb0-10e8-41ee-b271-7713924cabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.normal(train.Y,torch.sqrt(train.V) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1186a7-fa73-4909-b57a-be8f4cb2c932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = True \n",
    "from fitting.models import NNRBFKernel\n",
    "import pyro\n",
    "num_iter =  200\n",
    "num_particles =  256\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=normalized_train_data.toGpu()\n",
    "    #train=regression.sendToGpu(train_data)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "\n",
    "train.Y=torch.normal(train.Y,torch.sqrt(train.V) / 2)\n",
    "\n",
    "shape = (128,32)\n",
    "nnrbf = SK(NNRBFKernel(odim=2,layer_sizes=shape))\n",
    "\n",
    "if use_cuda and False:\n",
    "    smk = smk.cuda()\n",
    "    nnsmk = nnsmk.cuda()\n",
    "    smk.initialize_from_data(train.X, train.Y)\n",
    "    nnsmk.initialize_from_data(nnsmk.feature_extractor(train.X),train.Y)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train.V, learn_additional_noise=True)\n",
    "\n",
    "# model=models.InducingPointModel(train.X,train.Y, \n",
    "#                                 likelihood, \n",
    "#                                 kernel=nnrbf, \n",
    "#                                 inducing=train.X)\n",
    "\n",
    "rbf=SK(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "grbf=SK(models.GeneralRBF(ard_num_dims=2))\n",
    "\n",
    "#model=models.VariationalAnyKernelModel(kernel=rbf,inducing_points=train.X[::])\n",
    "#model = GPModel(inducing_points=train.X,likelihood=likelihood,)\n",
    "#model= ApproximateGPModel(train.X)\n",
    "\n",
    "#model=GPRegressionModel(train.X,train.Y,likelihood)\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "model,likelihood = regression.createModel(train, kernel=nnrbf)\n",
    "\n",
    "#model = PVGPRegressionModel(train.X, train.Y, likelihood)\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:       \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    print(\"Loaded model to gpu\")\n",
    "\n",
    "\n",
    "model,likelihood = regression.optimizeHyperparams(model,likelihood, \n",
    "                                                 train, \n",
    "                                                 mll=None, \n",
    "                                                 bar=False, \n",
    "                                                 iterations=800, lr=0.05)\n",
    "\n",
    "\n",
    "def trainModelX(lr=0.01):\n",
    "    optimizer = pyro.optim.Adam({\"lr\": 0.1})\n",
    "    elbo = pyro.infer.Trace_ELBO(num_particles=num_particles, vectorize_particles=True, retain_graph=True)\n",
    "    \n",
    "    svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n",
    "    model.train()\n",
    "\n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        model.zero_grad()\n",
    "        loss = svi.step(train.X, train.Y)\n",
    "        if i % 20 == 0:\n",
    "            print(loss)\n",
    "\n",
    "\n",
    "def trainModel():\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    ], lr=0.007)\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train.Y.size(0))\n",
    "    epochs_iter = range(1000)\n",
    "    for i in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        optimizer.zero_grad()        \n",
    "        output = model(train.X)\n",
    "        loss = -mll(output, train.Y)\n",
    "        loss.backward()\n",
    "        if i % 20 == 0:\n",
    "            print(loss)\n",
    "        optimizer.step()\n",
    "#trainModel()\n",
    "print(\"DONE\")\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model from gpu\")\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()\n",
    "    print(\"Loaded model from gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd68c8e-884e-4dc4-a6a4-bd1d12d7f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if mask_f:\n",
    "  #  mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "raw_pred_dist = regression.getPrediction(model, likelihood, normalized_test_data)\n",
    "with  gpytorch.settings.cholesky_max_tries(30):\n",
    "    pred_dist = futil.fixMVN(raw_pred_dist)\n",
    "\n",
    "\n",
    "slope=train_transform.transform_y.slope\n",
    "intercept = train_transform.transform_y.intercept\n",
    "print(slope,intercept)\n",
    "\n",
    "pred_dist = futil.affineTransformMVN(pred_dist,slope,  intercept)\n",
    "#pred_dist = likelihood(pred_dist)\n",
    "#pred_tr = train_transform.iTransform(regression.DataValues(normalized_test_data.X, pred_dist.mean, pred_dist.variance.detach(), normalized_test_data.E))\n",
    "pred = regression.DataValues(test_data.X, pred_dist.mean, pred_dist.variance.detach(), test_data.E)\n",
    "\n",
    "#print(all(pred_tr.Y == pred.Y))\n",
    "\n",
    "if mask_f:\n",
    "    mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "    bpred_mean = pred.Y[mask]\n",
    "    obs_mean = test_data.Y[mask]\n",
    "    print(f\"Total is {torch.sum(obs_mean)} -- {torch.sqrt(torch.sum(obs_mean))}\")\n",
    "    print(f\"Approx Sigma is {torch.sum(signal_data.Y[mask])/torch.sqrt(torch.sum(obs_mean))}\")\n",
    "    obs_var = test_data.V[mask]\n",
    "\n",
    "    chi2 = torch.sum((obs_mean - bpred_mean)**2 / obs_var) / torch.count_nonzero(mask)\n",
    "\n",
    "    print(f\"Global Chi^2/bins = {chi2}\")\n",
    "\n",
    "    chi2 = torch.sum((obs_mean - bpred_mean)**2 / obs_var) / torch.count_nonzero(mask)\n",
    "    avg_pull = torch.sum(torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)) / torch.count_nonzero(mask)\n",
    "    print(f\"Chi^2/bins = {chi2}\")\n",
    "    print(f\"Avg Abs pull = {avg_pull}\")\n",
    "else:\n",
    "    mask=None\n",
    "if avg_pull < 5 or True:    \n",
    "    p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6670f-82b4-4c86-8777-59f1a3914af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_pred_dist = pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67596a1c-a585-4b35-9fca-77abbfcf1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mean = old_pred_dist.mean\n",
    "new_mean = pred_dist.mean\n",
    "(old_mean-new_mean)**2/pred_dist.variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48320e-15e0-4804-833a-9f484ada2b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7fe2b-121e-44ac-b988-2eba91303eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCovarPlot(target, points, model):\n",
    "    return model(target, points)\n",
    "x = makeCovarPlot(\n",
    "    train_transform.transform_x.transformData(torch.tensor([[1500,0.45]])), train_transform.transform_x.transformData(pred.X), model.covar_module)\n",
    "x = torch.squeeze(x.to_dense().detach())\n",
    "fig,ax=plt.subplots()\n",
    "fplt.simpleGrid(ax, pred.E, pred.X, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea0dc5-f738-4733-87eb-5189ffb6ed10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280093b-0950-4fbc-90f8-2eb5b38190aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters_and_constraints())\n",
    "for name,param,constraint in params:\n",
    "    if constraint:\n",
    "        real_param = constraint.transform(param)\n",
    "    else:\n",
    "        real_param = param\n",
    "    print(f\"{name.replace('raw_',''):{max(len(x[0]) for x in params)+4}} {real_param.detach().numpy().round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8a99a-e78c-406b-9e95-5aaa9bdc1a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b5f46-2136-418f-b9ed-64bfcad63150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC\n",
    "from pyro.infer import MCMC, NUTS, Predictive\n",
    "import pyro.infer.reparam as pir\n",
    "import pyro.poutine as poutine\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoDelta\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "pyro.clear_param_store()\n",
    "background = pred_dist.mean\n",
    "scale=1e5\n",
    "background_dist = pred_dist\n",
    "#tm = test_transform.transform_y.iTransformData(pred_dist.mean)\n",
    "#tc = test_transform.transform_y.iTransformVariances(pred_dist.covariance_matrix)\n",
    "e = linear_operator.utils.cholesky.psd_safe_cholesky(pred_dist.covariance_matrix)\n",
    "e = torch.linalg.cholesky(pred_dist.covariance_matrix)\n",
    "assert(torch.allclose(pred_dist.covariance_matrix, e @ e.T))\n",
    "cm = e @ e.T\n",
    "mu = pred_dist.mean\n",
    "background_dist = dist.MultivariateNormal(mu,cm )\n",
    "if True:\n",
    "    print(torch.allclose(cm, e @ e.T))\n",
    "    vals,vecs = torch.linalg.eigh(cm)\n",
    "    vals=vals.real\n",
    "    vecs = vecs.real\n",
    "    print(torch.isreal(vals))\n",
    "    X = vecs @ torch.diag(torch.sqrt(vals))\n",
    "    assert(torch.allclose(X @ X.T, cm))\n",
    "    print(background_dist)\n",
    "    \n",
    "    eva = vals[:]\n",
    "    eve = vecs[:].T\n",
    "    \n",
    "    evam = torch.sqrt(torch.diag(eva))\n",
    "    #print(jnp.diag(eva).shape)\n",
    "    #print(eve.shape)\n",
    "    X= torch.matmul(eve, evam)\n",
    "    print(X.shape)\n",
    "    X=e\n",
    "    #@pir.AutoReparam()\n",
    "    assert(torch.allclose(X @ X.T, cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850e811-008f-4df8-a06a-d06800c37222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "use_variation_method = True\n",
    "signal_dist = signal_data.Y\n",
    "def pyro_model():           \n",
    "    r= pyro.sample(\"rate\",dist.Uniform(-20,20))  \n",
    "    if use_variation_method:\n",
    "        with pyro.plate(\"background_variations\", X.shape[1]):  \n",
    "            b = pyro.sample(\"raw_variations\", dist.Normal(0,1))\n",
    "        background = mu + X @ b \n",
    "    else:\n",
    "        background = pyro.sample(\"background_dist\", background_dist)\n",
    "    # print(\"==============================================\")\n",
    "    # print(background)\n",
    "    # print(background_dist.sample())\n",
    "    # print(mu)\n",
    "    # print(\"==============================================\")    \n",
    "    obs_hist = (r * signal_dist) + background\n",
    "    with pyro.plate(\"bins\", mu.shape[0]):    \n",
    "        #return pyro.sample(\"observed\", dist.Normal(torch.clamp(obs_hist,1), torch.sqrt(torch.clamp(obs_hist,1))))\n",
    "        return pyro.sample(\"observed\", dist.Poisson(torch.clamp(obs_hist,1)))\n",
    "\n",
    "def fake_model():           \n",
    "    obs_hist = mu\n",
    "    with pyro.plate(\"bins\", mu.shape[0]):    \n",
    "        #return pyro.sample(\"observed\", dist.Normal(torch.clamp(obs_hist,1), torch.sqrt(torch.clamp(obs_hist,1))))\n",
    "        return pyro.sample(\"observed\", dist.Poisson(torch.clamp(obs_hist,1)))\n",
    "\n",
    "#obs = train_transform.transform_y.iTransformData(mu) +  0 * signal_dist\n",
    "obs = mu +  1 * signal_dist\n",
    "\n",
    "obs = torch.ceil(torch.clamp(obs,1))\n",
    "\n",
    "conditioned = pyro.condition(pyro_model, {\"observed\": obs})\n",
    "if False:\n",
    "    fig,ax = plt.subplots(2)\n",
    "    ax[0].hist([pyro_model()[0] for x in range(2000)])\n",
    "    ax[1].hist([fake_model()[0] for x in range(2000)])\n",
    "\n",
    "\n",
    "p = pyro.render_model(conditioned,\n",
    "    render_params=True,\n",
    "    render_distributions=True,\n",
    "    render_deterministic=True)\n",
    "guide = pyro.infer.autoguide.guides.AutoLowRankMultivariateNormal(conditioned)\n",
    "p = Predictive(pyro_model, num_samples=1000,)()\n",
    "f = Predictive(fake_model, num_samples=1000,)()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de6e5e-d600-4063-90f5-b15e2bb516cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.std(p[\"observed\"][0]))\n",
    "print(torch.std(f[\"observed\"][0]))\n",
    "print(pred_dist.variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375df2e-cc07-4955-904b-9fe0eb8015bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(conditioned, guide, adam, loss=Trace_ELBO())\n",
    "num=1000\n",
    "for j in range(num):    # calculate the loss and take a gradient step\n",
    "    loss = svi.step()\n",
    "    if j % (num // 5) == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f437825-cfe6-4548-a07c-9fdc0d7a95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats\n",
    "predictive = Predictive(pyro_model, guide=guide,num_samples=800,)\n",
    "samples = predictive()\n",
    "pred_summary = summary(samples)\n",
    "\n",
    "print(pred_summary['rate']['mean'],pred_summary['rate']['std'] )\n",
    "#print(train_transform.transform_y.iTransformData(mu))\n",
    "#print(train_transform.transform_y.iTransformData(mu + X @ pred_summary[\"raw_variations\"][\"mean\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b783b-e81c-4109-b69b-437167e125fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "def hook(k,s,stage,i):\n",
    "    return \n",
    "    print(s)\n",
    "\n",
    "nuts_kernel = NUTS(conditioned,adapt_step_size=True)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=800, warmup_steps=400, hook_fn=hook)\n",
    "mcmc.run()\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c08b6-d782-4456-8278-18fc9fadea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfe630-1154-4446-b75c-d058807d371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "r = mcmc.get_samples()[\"rate\"]\n",
    "l = r.shape[0]\n",
    "p = torch.sum(r > 3.05) / l\n",
    "torch.distributions.Normal(0,1).icdf(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10435986-b68d-43da-a06f-e0c0f03be5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211dbcb0-cb77-4791-b317-a50911286dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "data = az.from_pyro(mcmc)\n",
    "\n",
    "one_rate = data.posterior[\"rate\"]\n",
    "ax = az.plot_trace(one_rate)\n",
    "az.plot_trace(zero_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abbc7f-78d3-4c20-8e0f-3c1f3015d1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce750c-42d3-44e6-90f1-ecc43fb9e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix,ax=plt.subplots()\n",
    "ax.hist(one_rate.to_numpy().flatten(),alpha=0.5)\n",
    "ax.hist(zero_rate.to_numpy().flatten(),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0540a0b-4aa4-492c-ad6a-08c8c1c0bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = torch.median(points, axis=0).values\n",
    "    print(median)\n",
    "    diff = torch.sum((points - median)**2, axis=-1)\n",
    "    diff = torch.sqrt(diff)\n",
    "    med_abs_deviation = torch.median(diff)\n",
    "    print(med_abs_deviation)\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "    return modified_z_score > thresh\n",
    "def noOutliers(a, thresh = 3.5):\n",
    "    return a[~is_outlier(a,thresh)]\n",
    "    \n",
    "s = mcmc.get_samples()\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.hist(noOutliers(s['rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1ce66-072f-4577-ba7f-eb541df20b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = poutine.trace(pyro_model)\n",
    "t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6897e6-6258-4385-be1a-74a15b2cdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "guide = pyro.infer.autoguide.AutoNormal(conditioned)\n",
    "print(guide)\n",
    "#def guide():\n",
    "#    pass\n",
    "num_steps = 4000\n",
    "initial_lr = 0.1\n",
    "gamma = 0.01  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "adam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "#adam = pyro.optim.Adam({\"lr\": 0.01})  # Consider decreasing learning rate.\n",
    "elbo = pyro.infer.Trace_ELBO()\n",
    "svi = pyro.infer.SVI(conditioned, guide, adam, elbo)\n",
    "losses = []\n",
    "for step in range(num_steps):  # Consider running for more steps.\n",
    "    loss = svi.step()\n",
    "    losses.append(loss)\n",
    "    if step % ( num_steps // 10) == 0:\n",
    "        #print(f\"r = {pyro.param('r').item()}\")\n",
    "        print(\"Elbo loss: {:0.3f}\".format(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d715b90-5792-4be9-82ce-3d4c3781eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "predictive = Predictive(conditioned, guide=guide, num_samples=1000)\n",
    "p = predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ec4d5-1c1e-48c8-ab54-b1153855e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = p['rate'].detach().flatten()\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(r,bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad459a-c0e9-4e98-879a-5ac5fca67f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13ce1f-ab59-4920-9320-edcb9b50aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitting.plot_tools as fpt\n",
    "import analyzer.plotting as plotting\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import fitting\n",
    "\n",
    "importlib.reload(fpt)\n",
    "dim = 1\n",
    "\n",
    "pred_mean,_ = fitting.regression.pointsToGrid(test_data.X, pred.Y, test_data.E)\n",
    "pred_var,_ = fitting.regression.pointsToGrid(test_data.X, pred.V, test_data.E)\n",
    "obs_vals,_ = fitting.regression.pointsToGrid(test_data.X, test_data.Y, test_data.E)\n",
    "obs_vars, filled = fitting.regression.pointsToGrid(test_data.X, test_data.V, test_data.E)\n",
    "\n",
    "#(figpath /\"slices\" / f\"along_{dim}\").mkdir(parents=True, exist_ok=True)\n",
    "for val, f, ax in fpt.createSlices(\n",
    "    pred_mean.hist,\n",
    "    pred_var.hist,\n",
    "    obs_vals.hist,\n",
    "    obs_vars.hist,\n",
    "    test_data.E,\n",
    "    filled,\n",
    "    mask_function=mask_f,\n",
    "    observed_title=\"CRData\", slice_dim=dim, just_window=True):\n",
    "    plotting.addTitles1D(ax, plotting.PlotObject.fromHist(qcd_hist[:,sum]))\n",
    "    #f.savefig(figpath /\"slices\" / f\"along_{dim}\" /  (f\"slice_{round(float(val),3)}\".replace(\".\",\"p\") + \".pdf\"))\n",
    "    #plt.close(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b85528-bb0a-434b-a448-f50ddf5c8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ApproximateGP, GP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=normalized_train_data.toGpu()\n",
    "    test=normalized_test_data.toGpu()\n",
    "    #train = regression.DataValues(normalized_train_data.X.cuda(), normalized_train_data.Y.cuda(), normalized_train_data.V.cuda(), None)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "    test = normalized_train_data\n",
    "\n",
    "smoke_test = True\n",
    "train_x, train_y, train_v = train.X, train.Y, train.V\n",
    "test_x, test_y, test_v = test.X, test.Y, test.V\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y, train_v)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100000, shuffle=True)\n",
    "\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant'):\n",
    "        if output_dims is None:\n",
    "            inducing_points = torch.randn(num_inducing, input_dims) + 1\n",
    "            batch_shape = torch.Size([])\n",
    "        else:\n",
    "            inducing_points = torch.randn(output_dims, num_inducing, input_dims) + 1\n",
    "            batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        if mean_type == 'constant':\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        else:\n",
    "            self.mean_module = LinearMean(input_dims)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape, ard_num_dims=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, *other_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily. For example, hidden_layer2(hidden_layer1_outputs, inputs) will pass the concatenation of the first\n",
    "        hidden layer's outputs and the input data to hidden_layer2.\n",
    "        \"\"\"\n",
    "        if len(other_inputs):\n",
    "            if isinstance(x, gpytorch.distributions.MultitaskMultivariateNormal):\n",
    "                x = x.rsample()\n",
    "\n",
    "            processed_inputs = [\n",
    "                inp.unsqueeze(0).expand(gpytorch.settings.num_likelihood_samples.value(), *inp.shape)\n",
    "                for inp in other_inputs\n",
    "            ]\n",
    "\n",
    "            x = torch.cat([x] + processed_inputs, dim=-1)\n",
    "        return super().__call__(x, are_samples=bool(len(other_inputs)))\n",
    "\n",
    "num_hidden_dims =  10\n",
    "\n",
    "class DeepGP(DeepGP):\n",
    "    def __init__(self, train_x_shape, train_vars):\n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=num_hidden_dims,\n",
    "            mean_type='linear',\n",
    "        )\n",
    "        \n",
    "        middle_hidden = ToyDeepGPHiddenLayer(\n",
    "            input_dims=num_hidden_dims,\n",
    "            output_dims=num_hidden_dims,\n",
    "            mean_type='linear',\n",
    "        )\n",
    "\n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=middle_hidden.output_dims,\n",
    "            output_dims=None,\n",
    "            mean_type='constant',\n",
    "        )\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.last_layer = last_layer\n",
    "        self.middle_hidden = middle_hidden\n",
    "        self.likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train_vars)\n",
    "        #self.likelihood = gpytorch.likelihoods.GaussianLi(noise=train_vars)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer(inputs)\n",
    "        x = self.middle_hidden(hidden_rep1)\n",
    "        output = self.last_layer(hidden_rep1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            mus = []\n",
    "            variances = []\n",
    "            lls = []\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                preds = self.likelihood(self(x_batch))\n",
    "                mus.append(preds.mean)\n",
    "                variances.append(preds.variance)\n",
    "                lls.append(model.likelihood.log_marginal(y_batch, model(x_batch)))\n",
    "        \n",
    "        return torch.cat(mus, dim=-1).mean(0), torch.cat(variances, dim=-1).mean(0), torch.cat(lls, dim=-1)\n",
    "\n",
    "model = DeepGP(train_x.shape,train_v)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "num_epochs =  5000\n",
    "num_samples = 4\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},], lr=0.05)\n",
    "mll = DeepApproximateMLL(\n",
    "    gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model))\n",
    "\n",
    "epochs_iter = range(num_epochs)\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data    minibatch_iter = train_loader\n",
    "    \n",
    "    #for x_batch, y_batch, noise in train_loader:\n",
    "    with gpytorch.settings.num_likelihood_samples(num_samples):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y, noise=train_v)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % (num_epochs // 10) == 0 :\n",
    "        print(f\"Epoch {i:3}: Loss {loss:0.3f}\")\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e89c91-b95c-4a4d-8030-541b3e3943dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9a90e-7656-4694-8ece-f3a2206763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000000)\n",
    "\n",
    "model.eval()\n",
    "#predictive_means, predictive_variances, test_lls = model.predict(test_loader)\n",
    "with torch.no_grad(): \n",
    "    p = model.likelihood(model(test_x),noise=test_v)\n",
    "    predictive_means = p.mean.mean(0)\n",
    "    predictive_variances = p.variance.mean(0)\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    predictive_means=predictive_means.cpu()\n",
    "    predictive_variances=predictive_variances.cpu()\n",
    "    #train = regression.DataValues(normalized_train_data.X.cuda(), normalized_train_data.Y.cuda(), normalized_train_data.V.cuda(), None)\n",
    "\n",
    "pred = test_transform.iTransform(\n",
    "    regression.DataValues(normalized_test_data.X, predictive_means, predictive_variances, normalized_test_data.E))\n",
    "\n",
    "import fitting.high_level as fhl\n",
    "predictive_means\n",
    "\n",
    "p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c252f-b0f7-4584-a48e-4b4c0c45de5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349254f-c594-4833-ac07-34a12a3afcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026d8ce-1346-4d4a-be54-e24acca254f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "class PyroGPModel(gpytorch.models.PyroGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, inducing_points, kernel=None, mean=None):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(0),\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, \n",
    "            variational_distribution, \n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(\n",
    "            variational_strategy,\n",
    "            likelihood,\n",
    "            num_data=train_y.numel(),\n",
    "            name_prefix=\"simple_regression_model\"\n",
    "        )\n",
    "        self.likelihood = likelihood\n",
    "        self.mean_module = mean or gpytorch.means.ConstantMean()\n",
    "        if kernel is None:\n",
    "            kernel = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=2)\n",
    "            )\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)  # Returns an n_data vec\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points,kernel=None, mean=None):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, \n",
    "            inducing_points, \n",
    "            variational_distribution, \n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "        self.mean_module = mean or gpytorch.means.ConstantMean()\n",
    "        if kernel is None:\n",
    "            kernel = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "use_cuda = True \n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading train to gpu\")\n",
    "    train=regression.sendToGpu(normalized_train_data)\n",
    "    print(\"Loaded train to gpu\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "\n",
    "NNRBF = fitting.models.wrapNN(\"NNRBFKernel\", gpytorch.kernels.RBFKernel)\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=(256,128,64,32)))\n",
    "nnrbf = SK(NNRBF(odim=2,layer_sizes=(32,16)))\n",
    "\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train.V)\n",
    "model=StandardApproximateGP(\n",
    "                 #train.X,\n",
    "                 #train.Y, \n",
    "                 #likelihood, \n",
    "                 train.X[::4],\n",
    "                 kernel=grbf\n",
    "              )\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    print(\"Loaded model to gpu\")\n",
    "\n",
    "def trainModel(model, likelihood, lr=0.01, num_iter = 1):\n",
    "    optimizer = pyro.optim.Adam({\"lr\": lr})\n",
    "    elbo = pyro.infer.Trace_ELBO(num_particles=128, vectorize_particles=True, retain_graph=True)\n",
    "    svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        model.zero_grad()\n",
    "        loss = svi.step(train.X, train.Y)\n",
    "        if i % (num_iter // 10) == 0 :\n",
    "            print(f\"Iter {i}: Loss {loss:2}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def trainModel2(model, likelihood,lr=0.1,num_iter = 1):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=lr)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train.Y.size(0))\n",
    "    \n",
    "    \n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train.X)\n",
    "        loss = -mll(output, train.Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % (num_iter // 10) == 0 :\n",
    "            print(f\"Iter {i}: Loss {loss:2}\")\n",
    "    \n",
    "    print(f\"Iter {i}: Loss {loss:2}\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "print(model)\n",
    "model = trainModel2(model,likelihood,lr=0.025,num_iter=10000)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DONE\")\n",
    "#with linear_operator.settings.max_cg_iterations(2000):\n",
    "#    model,likelihood = regression.optimizeHyperparams(model,likelihood, train, bar=False, iterations=250, lr=0.05)\n",
    "print(\"DONE\")\n",
    "if torch.cuda.is_available() and use_cuda:   \n",
    "    print(\"Loading model to gpu\")\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()\n",
    "    print(\"Loaded model to gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b65fe-b7f6-4c1d-86db-4339535c8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters_and_constraints())\n",
    "for name,param,constraint in params:\n",
    "    if constraint:\n",
    "        real_param = constraint.transform(param)\n",
    "    else:\n",
    "        real_param = param\n",
    "    print(f\"{name.replace('raw_',''):{max(len(x[0]) for x in params)+4}} {real_param.detach().numpy().round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ab348-f4ee-4084-a80c-b60162c7648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_dist = regression.getPrediction(model, likelihood, normalized_test_data)\n",
    "pred = test_transform.iTransform(    regression.DataValues(normalized_test_data.X, pred_dist.mean, pred_dist.variance, normalized_test_data.E))\n",
    "\n",
    "if window:\n",
    "    mask = regression.getBlindedMask(pred.X, pred.Y, test_data.Y, test_data.V, mask_f)\n",
    "    bpred_mean = pred.Y[mask]\n",
    "    obs_mean = test_data.Y[mask]\n",
    "    obs_var = test_data.V[mask]\n",
    "    chi2 = torch.sum((obs_mean - bpred_mean)**2 / obs_var) / torch.count_nonzero(mask)\n",
    "    avg_pull = torch.sum(torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)) / torch.count_nonzero(mask)\n",
    "    print(f\"Chi^2/bins = {chi2}\")\n",
    "    print(f\"Avg Abs pull = {avg_pull}\")\n",
    "if avg_pull < 5:    \n",
    "    p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ab9cb-a86a-459d-a851-c80854aae2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = fhl.makeDiagnosticPlots(pred, test_data, train_data, qcd_hist, mask)\n",
    "_,ax1 = p[\"gpr_mean\"]\n",
    "indp=model.covar_module.inducing_points\n",
    "ind = test_transform.transform_x.iTransformData(indp).detach()\n",
    "\n",
    "#ax1.scatter(ind[:,0], ind[:,1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsmlenv",
   "language": "python",
   "name": "cmsmlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
