{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a0c62-b36a-4fb7-b223-5625da605abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c25665-dda0-4950-8690-ff7f954684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pyro\n",
    "import pickle as pkl\n",
    "from fitting.high_level import RegressionModel\n",
    "from fitting.regression import DataValues\n",
    "from fitting.high_level import makeDiagnosticPlots\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from fitting.utils import affineTransformMVN\n",
    "import pyro \n",
    "from fitting.high_level import RegressionModel\n",
    "from fitting.regression import DataValues, makeRegressionData\n",
    "from fitting.utils import getScaledEigenvecs, fixMVN, affineTransformMVN\n",
    "from fitting.transformations import getNormalizationTransform\n",
    "import fitting.models as models\n",
    "import fitting.regression as regression\n",
    "import fitting.transformations as transformations\n",
    "import fitting.windowing as windowing\n",
    "from gpytorch.kernels import ScaleKernel as SK\n",
    "import hist\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import pyro.distributions as pyrod\n",
    "import pyro.infer as pyroi\n",
    "\n",
    "\n",
    "from fitting.plot_tools import createSlices, getPolyFromSquares, makeSquares, simpleGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136487-745d-4890-8559-2989d4d90ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"regression_results/2018_Control_nn_uncomp_0p67_m14_vs_mChiUncompRatio.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    control = pkl.load(f)\n",
    "\n",
    "inhist = control[\"Data2018\", \"Control\"][\"hist_collection\"][\"histogram\"]#[hist.loc(1000) :, hist.loc(0) :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8c3c8-465b-4b6d-8e25-e1826bbbe553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(2, 32))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(32, 16))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(16, 8))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(8, 2))\n",
    "\n",
    "ft = LargeFeatureExtractor()\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            \n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "\n",
    "            self.feature_extractor = ft\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523599a-9ecd-4e8c-b837-845df1289ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFLayer(torch.nn.Module):\n",
    "    def __init__(self, dim, count):\n",
    "        super().__init__()\n",
    "        self.length_scales = torch.nn.Parameter(torch.ones(count))\n",
    "        self.centers = torch.nn.Parameter(torch.ones(count,dim))\n",
    "\n",
    "    def forward(self, vals):\n",
    "        return torch.exp((torch.unsqueeze(vals,1) - self.centers).pow(2).sum(-1)/(2 * self.length_scales))\n",
    "\n",
    "r = RBFLayer(2,3)       \n",
    "a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059], [-0.4821,  1.059], [-0.4821,  1.059], [-0.4821,  1.059]])\n",
    "l =  torch.nn.Linear(3,1)\n",
    "\n",
    "l(r(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a770b6a-0966-47e8-969b-6928ba2feadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStatKernel(gpytorch.kernels.RBFKernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = False\n",
    "    def __init__(self, dim=2, count=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pre_transform = RBFLayer(dim,count)\n",
    "        self.trans = torch.nn.Linear(count,1)\n",
    "        #self.add_module(\"trans\", self.trans)\n",
    "        #self.add_module(\"pre\", self.pre_transform)\n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "\n",
    "        v1 = self.trans(self.pre_transform(x1))\n",
    "        v2 = self.trans(self.pre_transform(x2))\n",
    "        r =  super().forward(x1,x2,diag=diag,**params)\n",
    "\n",
    "        if diag:\n",
    "            o = torch.squeeze(v1*v2) \n",
    "        else:\n",
    "            o =torch.outer(v1.squeeze(), v2.squeeze())\n",
    "\n",
    "        return o * r #, **params)\n",
    "        \n",
    "class NonStatGP(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, function=None):\n",
    "            super().__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "            self.base_covar_module = NonStatKernel(ard_num_dims=2)  #* NonStatKernel(ard_num_dims=2)\n",
    "            if False:\n",
    "                self.covar_module = self.base_covar_module                                                   \n",
    "            else:\n",
    "                self.covar_module = gpytorch.kernels.InducingPointKernel(self.base_covar_module,\n",
    "                            likelihood=likelihood,\n",
    "                             inducing_points=train_x[::2].clone())\n",
    "\n",
    "           # self.covar_module = SK(NonStatKernel(ard_num_dims=2))\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630696f1-84c9-45eb-a60a-31ab8e15c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSample(*args, size=None):\n",
    "    p = torch.randperm(args[0].size(0))\n",
    "    return [x[p[:size]] for x in args]\n",
    "\n",
    "def batchify(*args, size=None, batches=None):\n",
    "    vals = [randSample(*args, size=size) for i in range(batches)]\n",
    "    return tuple(torch.stack(x) for x in zip(*vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dc91e-a8ae-4430-969d-8b3e35c8c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2bins(obs,exp,var):\n",
    "    return torch.sum((obs-exp).pow(2) / var ) / obs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4e9db-0271-4286-b131-8f23b51a89a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def bumpCut(X, Y):\n",
    "    m = Y > (1 - 200 / X)\n",
    "    return m\n",
    "\n",
    "window_func = windowing.EllipseWindow([1300,0.5],[160,0.06])\n",
    "#window_func = windowing.EllipseWindow([0.5,0.5],[0.1,0.1])\n",
    "\n",
    "min_counts = 50\n",
    "train_data = regression.makeRegressionData(\n",
    "    inhist, window_func, domain_mask_function=bumpCut, exclude_less=min_counts\n",
    ")\n",
    "test_data, domain_mask = regression.makeRegressionData(\n",
    "    inhist,\n",
    "    None,\n",
    "    get_mask=True,\n",
    "    domain_mask_function=bumpCut,\n",
    "    exclude_less=min_counts,\n",
    ")\n",
    "train_transform = transformations.getNormalizationTransform(train_data)\n",
    "normalized_train_data = train_transform.transform(train_data)\n",
    "normalized_test_data = train_transform.transform(test_data)\n",
    "kernel = SK(gpytorch.kernels.RBFKernel(ard_num_dims=2)) \n",
    "# kernel = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4,ard_num_dims=2)\n",
    "#kernel = SK(gpytorch.kernels.MaternKernel(ard_num_dims=2)) * SK(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))* SK(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=2))\n",
    "use_cuda = True\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    train = normalized_train_data.toGpu()\n",
    "    norm_test = normalized_test_data.toGpu()\n",
    "    print(\"USING CUDA\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "    norm_test = normalized_test_data\n",
    "    \n",
    "\n",
    "#trainX, trainY, trainV = batchify(train.X,train.Y,train.V, size=800, batches=10)\n",
    "\n",
    "trainX, trainY, trainV = train.X, train.Y, train.V\n",
    "\n",
    "#kernel = SK(gpytorch.kernels.MaternKernel(ard_num_dims=2)) * SK(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n",
    "    noise=trainV*4,\n",
    "    learn_additional_noise=False,\n",
    ")\n",
    "\n",
    "#model = GPRegressionModel(trainX, trainY, likelihood)\n",
    "#model = NonStatGP(normalized_train_data.X, normalized_train_data.Y, likelihood,)\n",
    "model = models.ExactAnyKernelModel(trainX, trainY, likelihood, kernel=kernel)\n",
    "print(model)\n",
    "lr = 0.05\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "# if hasattr(model.covar_module, \"initialize_from_data\"):\n",
    "#     model.covar_module.initialize_from_data(train.X, train.Y)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(400):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(trainX)\n",
    "    # Calc loss and backprop gradients\n",
    "    l = -mll(output, trainY)\n",
    "    mse = gpytorch.metrics.mean_squared_error(output, trainY,  squared=True)\n",
    "    #print(l)\n",
    "    loss = l\n",
    "    loss.backward()\n",
    "    if i % 20 == 0 :     \n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, 200, loss.item()))\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "# model, likelihood, evidence = regression.optimizeHyperparams(\n",
    "#     model,\n",
    "#     likelihood,\n",
    "#     train,\n",
    "#     bar=False,\n",
    "#     iterations=800,\n",
    "#     lr=lr,\n",
    "#     get_evidence=True,\n",
    "# )\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f3576-d0f3-4388-b63b-4a02c28f6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module.base_kernel.lengthscale = torch.tensor([[0.1,0.1]])\n",
    "model.covar_module.outputscale = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b4928-0001-4b0e-bec6-6d52b783db17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dc2c5-4c5c-4549-8d01-460641866ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.round(decimals=2).tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9104c-8064-468e-9a44-4c35357b1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = regression.getPrediction(\n",
    "    model, likelihood, normalized_test_data\n",
    ")\n",
    "print(pred_dist)\n",
    "\n",
    "\n",
    "with gpytorch.settings.cholesky_max_tries(30):\n",
    "    psd_pred_dist = fixMVN(pred_dist)\n",
    "pred_dist = type(pred_dist)(\n",
    "    psd_pred_dist.mean, psd_pred_dist.covariance_matrix.to_dense()\n",
    ")\n",
    "slope = train_transform.transform_y.slope\n",
    "intercept = train_transform.transform_y.intercept\n",
    "pred_dist = affineTransformMVN(psd_pred_dist, slope, intercept)\n",
    "\n",
    "\n",
    "to_use = test_data\n",
    "to_use = regression.DataValues(\n",
    "    to_use.X,#.unsqueeze(0).repeat(4,1,1),\n",
    "    to_use.Y,#.unsqueeze(0).repeat(4,1),\n",
    "    to_use.V,#.unsqueeze(0).repeat(4,1),\n",
    "    to_use.E,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pred_data = regression.DataValues(\n",
    "    to_use.X,\n",
    "    pred_dist.mean,\n",
    "    pred_dist.variance,\n",
    "    to_use.E,\n",
    ")\n",
    "\n",
    "mask = regression.getBlindedMask(pred_data.X, pred_data.Y, to_use.Y, to_use.V, window_func)\n",
    "print(mask)\n",
    "#gbm = (slice(None),torch.ones_like( test_data.Y > 50))\n",
    "gbm = torch.ones_like(test_data.Y > 50)\n",
    "\n",
    "x =     torch.sum((pred_data.Y[gbm] - to_use.Y[gbm]) ** 2/ to_use.V[gbm], dim=0)\n",
    "global_chi2_bins = torch.sum((pred_data.Y[gbm] - to_use.Y[gbm]) ** 2/ to_use.V[gbm], dim=0)/ to_use.Y[gbm].shape[0]\n",
    "\n",
    "print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "ok = True\n",
    "\n",
    "\n",
    "mask = regression.getBlindedMask(\n",
    "    pred_data.X, pred_data.Y, test_data.Y, test_data.V, window_func\n",
    ")\n",
    "wm= (slice(None), mask)\n",
    "print(mask.shape)\n",
    "bpred_mean = pred_data.Y[mask]\n",
    "obs_mean = to_use.Y[mask]\n",
    "obs_var = to_use.V[mask]\n",
    "print(bpred_mean)\n",
    "chi2 = torch.sum((obs_mean - bpred_mean) ** 2 / obs_var) / torch.count_nonzero(\n",
    "    mask\n",
    ")\n",
    "avg_pull = torch.sum(\n",
    "    torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var),dim=0\n",
    ") / torch.count_nonzero(mask)\n",
    "\n",
    "print(f\"Avg Abs pull = {avg_pull}\")\n",
    "print(f\"Chi^2/bins = {chi2}\")\n",
    "\n",
    "print(f\"Global Chi2/bins = {global_chi2_bins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329a8ba-077a-4872-bee0-696516289d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model.covar_module, \"inducing_points\"):\n",
    "    i = model.covar_module.inducing_points.detach().numpy()\n",
    "else:\n",
    "    i = None\n",
    "plots = makeDiagnosticPlots(pred_data,\n",
    "                            test_data, \n",
    "                            train_data,\n",
    "                            mask,\n",
    "                            inducing_points=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabad09-371d-4887-a53c-7856ba989195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fe = model.feature_extractor\n",
    "#for param_name, param in model.named_parameters():\n",
    "#    print(f'Parameter name: {param_name:42} value = {param.round(decimals=2).tolist()}')\n",
    "\n",
    "s = model.scale_to_bounds\n",
    "def t(x):\n",
    "    return fe(x)\n",
    "\n",
    "T = t(test_data.X).detach()\n",
    "print(T[:,0] / T[:,1])\n",
    "fig,ax=plt.subplots()\n",
    "ax.scatter(T[:,0], T[:,1], c=test_data.Y, cmap='hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276b243-73e4-408b-8cb2-c120589a0f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5b78d-e873-4e48-9ccf-20040c7ad9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "   logger.info(\"Done with loop\")\n",
    "            print(\"Done with loop\")\n",
    "            raw_pred_dist = regression.getPrediction(\n",
    "                model, likelihood, normalized_test_data\n",
    "            )\n",
    "            with gpytorch.settings.cholesky_max_tries(30):\n",
    "                psd_pred_dist = fit_utils.fixMVN(raw_pred_dist)\n",
    "            raw_pred_dist = type(raw_pred_dist)(\n",
    "                psd_pred_dist.mean, psd_pred_dist.covariance_matrix.to_dense()\n",
    "            )\n",
    "            slope = train_transform.transform_y.slope\n",
    "            intercept = train_transform.transform_y.intercept\n",
    "            pred_dist = fit_utils.affineTransformMVN(psd_pred_dist, slope, intercept)\n",
    "\n",
    "            pred_data = regression.DataValues(\n",
    "                test_data.X,\n",
    "                pred_dist.mean,\n",
    "                pred_dist.variance,\n",
    "                test_data.E,\n",
    "            )\n",
    "\n",
    "            good_bin_mask = test_data.Y > 500\n",
    "            global_chi2_bins = float(\n",
    "                torch.sum(\n",
    "                    (pred_data.Y[good_bin_mask] - test_data.Y[good_bin_mask]) ** 2\n",
    "                    / test_data.V[good_bin_mask]\n",
    "                )\n",
    "                / test_data.Y[good_bin_mask].shape[0]\n",
    "            )\n",
    "            print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "            if global_chi2_bins < 1.5:\n",
    "                ok = True\n",
    "            else:\n",
    "                logger.warning(\"Bad global Chi2, retrying\")\n",
    "            ok = True\n",
    "\n",
    "        except (\n",
    "            linear_operator.utils.errors.NanError,\n",
    "            linear_operator.utils.errors.NotPSDError,\n",
    "        ) as e:\n",
    "            lr = lr + random.random() / 1000\n",
    "            logger.warning(f\"CHOLESKY FAILED: retrying with lr={round(lr,3)}\")\n",
    "            logger.warning(e)\n",
    "\n",
    "    logger.warning(\"Done training\")\n",
    "\n",
    "    data = {\n",
    "        \"evidence\": evidence,\n",
    "        \"global_chi2/bins\": global_chi2_bins,\n",
    "        \"model_string\": str(model),\n",
    "    }\n",
    "\n",
    "    if window_func:\n",
    "        mask = regression.getBlindedMask(\n",
    "            pred_data.X, pred_data.Y, test_data.Y, test_data.V, window_func\n",
    "        )\n",
    "        bpred_mean = pred_data.Y[mask]\n",
    "        obs_mean = test_data.Y[mask]\n",
    "        obs_var = test_data.V[mask]\n",
    "        chi2 = torch.sum((obs_mean - bpred_mean) ** 2 / obs_var) / torch.count_nonzero(\n",
    "            mask\n",
    "        )\n",
    "        avg_pull = torch.sum(\n",
    "            torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)\n",
    "        ) / torch.count_nonzero(mask)\n",
    "\n",
    "        data.update(\n",
    "            {\n",
    "                \"chi2_blinded\": float(chi2),\n",
    "                \"avg_abs_pull\": float(avg_pull),\n",
    "            }\n",
    "        )\n",
    "        print(f\"Avg Abs pull = {avg_pull}\")\n",
    "        print(f\"Chi^2/bins = {chi2}\")\n",
    "    else:\n",
    "        mask = None\n",
    "    print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if True:\n",
    "        diagnostic_plots = makeDiagnosticPlots(pred_data, test_data, train_data, mask)\n",
    "        saveDiagnosticPlots(diagnostic_plots, save_dir)\n",
    "        # makeSlicePlots(pred_data, test_data, inhist, window_func, 0, save_dir)\n",
    "        # makeSlicePlots(pred_data, test_data, inhist, window_func, 1, save_dir)\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    save_data = RegressionModel(\n",
    "        input_data=inhist,\n",
    "        window=window_func,\n",
    "        domain_mask=domain_mask,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        trained_model=model,\n",
    "        raw_posterior_dist=raw_pred_dist,\n",
    "        posterior_dist=pred_dist,\n",
    "    )\n",
    "    torch.save(save_data, save_dir / \"train_model.pth\")\n",
    "    torch.save(save_data, save_dir / \"model_dict.pth\")\n",
    "    torch.save(pred_dist, save_dir / \"posterior_latent.pth\")\n",
    "    with open(save_dir / \"info.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ca4a7-4145-4645-bbad-728bd596a163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
