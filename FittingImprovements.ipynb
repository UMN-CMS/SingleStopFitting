{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a0c62-b36a-4fb7-b223-5625da605abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c25665-dda0-4950-8690-ff7f954684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pyro\n",
    "import pickle as pkl\n",
    "from fitting.high_level import RegressionModel\n",
    "from fitting.regression import DataValues\n",
    "from fitting.high_level import makeDiagnosticPlots\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from fitting.utils import affineTransformMVN\n",
    "import pyro \n",
    "from fitting.high_level import RegressionModel\n",
    "from fitting.regression import DataValues, makeRegressionData\n",
    "from fitting.utils import getScaledEigenvecs\n",
    "from fitting.transformations import getNormalizationTransform\n",
    "import fitting.models as models\n",
    "import fitting.regression as regression\n",
    "import fitting.transformations as transformations\n",
    "import fitting.windowing as windowing\n",
    "from gpytorch.kernels import ScaleKernel as SK\n",
    "import hist\n",
    "\n",
    "\n",
    "import pyro.distributions as pyrod\n",
    "import pyro.infer as pyroi\n",
    "\n",
    "\n",
    "from fitting.plot_tools import createSlices, getPolyFromSquares, makeSquares, simpleGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136487-745d-4890-8559-2989d4d90ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"regression_results/2018_Control_nn_uncomp_0p67_m14_vs_mChiUncompRatio.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    control = pkl.load(f)\n",
    "\n",
    "inhist = control[\"Data2018\", \"Control\"][\"hist_collection\"][\"histogram\"]#[hist.loc(1000) :, hist.loc(0) :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8c3c8-465b-4b6d-8e25-e1826bbbe553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(2, 64))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(64, 16))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(16, 8))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(8, 2))\n",
    "\n",
    "ft = LargeFeatureExtractor()\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            if False:\n",
    "                self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "            else:\n",
    "                self.covar_module = gpytorch.kernels.InducingPointKernel(\n",
    "                    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                                                                        likelihood=likelihood,\n",
    "                                                                         inducing_points = train_x[::2].clone()\n",
    "                                                                        )\n",
    "\n",
    "            self.feature_extractor = ft\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9dcc6-33b7-4550-a2c9-b0ec6fd57387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStatKernel(gpytorch.kernels.RBFKernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = False\n",
    "    def __init__(self, pre_transform=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pre_transform = pre_transform\n",
    "        self.trans = torch.nn.Linear(2,1)\n",
    "        self.add_module(\"trans\", self.trans)\n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        if self.pre_transform is not None:\n",
    "            x1 = self.pre_transform(x1)\n",
    "            x2 = self.pre_transform(x2)\n",
    "\n",
    "        v1 = self.trans(x1)\n",
    "        v2 = self.trans(x2)\n",
    "\n",
    "        r =  super().forward(x1,x2,diag=diag,**params)\n",
    "\n",
    "        if diag:\n",
    "            o = torch.squeeze(v1*v2) \n",
    "        else:\n",
    "            o =torch.outer(v1.squeeze(), v2.squeeze())\n",
    "        return o * r #, **params)\n",
    "        \n",
    "class NonStatGP(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, function=None):\n",
    "            super().__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "            self.base_covar_module = SK(NonStatKernel(ard_num_dims=2))  #* NonStatKernel(ard_num_dims=2) \n",
    "            if False:\n",
    "                self.covar_module = self.base_covar_module                                                   \n",
    "            else:\n",
    "                self.covar_module = gpytorch.kernels.InducingPointKernel(self.base_covar_module,\n",
    "                            likelihood=likelihood,\n",
    "                             inducing_points=train_x[::2].clone())\n",
    "\n",
    "           # self.covar_module = SK(NonStatKernel(ard_num_dims=2))\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4e9db-0271-4286-b131-8f23b51a89a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bumpCut(X, Y):\n",
    "    m = Y > (1 - 200 / X)\n",
    "    return m\n",
    "\n",
    "window_func = windowing.EllipseWindow([0.4,0.4],[0.1,0.1])\n",
    "\n",
    "\n",
    "min_counts = 50\n",
    "train_data = regression.makeRegressionData(\n",
    "    inhist, window_func, domain_mask_function=bumpCut, exclude_less=min_counts\n",
    ")\n",
    "test_data, domain_mask = regression.makeRegressionData(\n",
    "    inhist,\n",
    "    None,\n",
    "    get_mask=True,\n",
    "    domain_mask_function=bumpCut,\n",
    "    exclude_less=min_counts,\n",
    ")\n",
    "train_transform = transformations.getNormalizationTransform(train_data)\n",
    "normalized_train_data = train_transform.transform(train_data)\n",
    "normalized_test_data = train_transform.transform(test_data)\n",
    "kernel = SK(gpytorch.kernels.RBFKernel(ard_num_dims=2)) \n",
    "# kernel = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4,ard_num_dims=2)\n",
    "#kernel = SK(gpytorch.kernels.MaternKernel(ard_num_dims=2)) * SK(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))* SK(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=2))\n",
    "\n",
    "#kernel = SK(gpytorch.kernels.MaternKernel(ard_num_dims=2)) * SK(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n",
    "    noise=normalized_train_data.V,\n",
    "    learn_additional_noise=False,\n",
    ")\n",
    "model = models.ExactAnyKernelModel(\n",
    "     normalized_train_data.X, normalized_train_data.Y, likelihood, kernel=kernel\n",
    ")\n",
    "# model = models.InducingPointModel(\n",
    "#      normalized_train_data.X, normalized_train_data.Y,\n",
    "#     likelihood, kernel=kernel,\n",
    "#      inducing=normalized_train_data.X[::2]\n",
    "# )\n",
    "model = GPRegressionModel(normalized_train_data.X, normalized_train_data.Y, likelihood,)\n",
    "# model = NonStatGP(normalized_train_data.X, normalized_train_data.Y, likelihood,)\n",
    "print(model)\n",
    "use_cuda = True\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    train = normalized_train_data.toGpu()\n",
    "    norm_test = normalized_test_data.toGpu()\n",
    "    print(\"USING CUDA\")\n",
    "else:\n",
    "    train = normalized_train_data\n",
    "    norm_test = normalized_test_data\n",
    "\n",
    "lr = 0.1\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "if hasattr(model.covar_module, \"initialize_from_data\"):\n",
    "    model.covar_module.initialize_from_data(train.X, train.Y)\n",
    "\n",
    "model, likelihood, evidence = regression.optimizeHyperparams(\n",
    "    model,\n",
    "    likelihood,\n",
    "    train,\n",
    "    bar=False,\n",
    "    iterations=800,\n",
    "    lr=lr,\n",
    "    get_evidence=True,\n",
    ")\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    model = model.cpu()\n",
    "    likelihood = likelihood.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dc2c5-4c5c-4549-8d01-460641866ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.round(decimals=2).tolist()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9104c-8064-468e-9a44-4c35357b1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = regression.getPrediction(\n",
    "    model, likelihood, normalized_test_data\n",
    ")\n",
    "\n",
    "\n",
    "# with gpytorch.settings.cholesky_max_tries(30):\n",
    "#     psd_pred_dist = fit_utils.fixMVN(raw_pred_dist)\n",
    "# raw_pred_dist = type(raw_pred_dist)(\n",
    "#     psd_pred_dist.mean, psd_pred_dist.covariance_matrix.to_dense()\n",
    "# )\n",
    "# slope = train_transform.transform_y.slope\n",
    "# intercept = train_transform.transform_y.intercept\n",
    "# pred_dist = fit_utils.affineTransformMVN(psd_pred_dist, slope, intercept)\n",
    "\n",
    "pred_data = regression.DataValues(\n",
    "    normalized_test_data.X,\n",
    "    pred_dist.mean,\n",
    "    pred_dist.variance,\n",
    "    normalized_test_data.E,\n",
    ")\n",
    "mask = regression.getBlindedMask(\n",
    "            pred_data.X, pred_data.Y, normalized_test_data.Y, normalized_test_data.V, window_func\n",
    "        )\n",
    "\n",
    "good_bin_mask = test_data.Y > 100\n",
    "global_chi2_bins = float(\n",
    "    torch.sum(\n",
    "        (pred_data.Y[good_bin_mask] - normalized_test_data.Y[good_bin_mask]) ** 2\n",
    "        / normalized_test_data.V[good_bin_mask]\n",
    "    )\n",
    "    / normalized_test_data.Y[good_bin_mask].shape[0]\n",
    ")\n",
    "print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329a8ba-077a-4872-bee0-696516289d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model.covar_module, \"inducing_points\"):\n",
    "    i = model.covar_module.inducing_points.detach().numpy()\n",
    "else:\n",
    "    i = None\n",
    "plots = makeDiagnosticPlots(pred_data,\n",
    "                            normalized_test_data, \n",
    "                            normalized_train_data,\n",
    "                            mask,\n",
    "                            inducing_points=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabad09-371d-4887-a53c-7856ba989195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fe = model.feature_extractor\n",
    "#for param_name, param in model.named_parameters():\n",
    "#    print(f'Parameter name: {param_name:42} value = {param.round(decimals=2).tolist()}')\n",
    "\n",
    "s = model.scale_to_bounds\n",
    "def t(x):\n",
    "    return fe(x)\n",
    "\n",
    "T = t(test_data.X).detach()\n",
    "print(T[:,0] / T[:,1])\n",
    "fig,ax=plt.subplots()\n",
    "ax.scatter(T[:,0], T[:,1], c=test_data.Y, cmap='hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276b243-73e4-408b-8cb2-c120589a0f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5b78d-e873-4e48-9ccf-20040c7ad9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "   logger.info(\"Done with loop\")\n",
    "            print(\"Done with loop\")\n",
    "            raw_pred_dist = regression.getPrediction(\n",
    "                model, likelihood, normalized_test_data\n",
    "            )\n",
    "            with gpytorch.settings.cholesky_max_tries(30):\n",
    "                psd_pred_dist = fit_utils.fixMVN(raw_pred_dist)\n",
    "            raw_pred_dist = type(raw_pred_dist)(\n",
    "                psd_pred_dist.mean, psd_pred_dist.covariance_matrix.to_dense()\n",
    "            )\n",
    "            slope = train_transform.transform_y.slope\n",
    "            intercept = train_transform.transform_y.intercept\n",
    "            pred_dist = fit_utils.affineTransformMVN(psd_pred_dist, slope, intercept)\n",
    "\n",
    "            pred_data = regression.DataValues(\n",
    "                test_data.X,\n",
    "                pred_dist.mean,\n",
    "                pred_dist.variance,\n",
    "                test_data.E,\n",
    "            )\n",
    "\n",
    "            good_bin_mask = test_data.Y > 500\n",
    "            global_chi2_bins = float(\n",
    "                torch.sum(\n",
    "                    (pred_data.Y[good_bin_mask] - test_data.Y[good_bin_mask]) ** 2\n",
    "                    / test_data.V[good_bin_mask]\n",
    "                )\n",
    "                / test_data.Y[good_bin_mask].shape[0]\n",
    "            )\n",
    "            print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "            if global_chi2_bins < 1.5:\n",
    "                ok = True\n",
    "            else:\n",
    "                logger.warning(\"Bad global Chi2, retrying\")\n",
    "            ok = True\n",
    "\n",
    "        except (\n",
    "            linear_operator.utils.errors.NanError,\n",
    "            linear_operator.utils.errors.NotPSDError,\n",
    "        ) as e:\n",
    "            lr = lr + random.random() / 1000\n",
    "            logger.warning(f\"CHOLESKY FAILED: retrying with lr={round(lr,3)}\")\n",
    "            logger.warning(e)\n",
    "\n",
    "    logger.warning(\"Done training\")\n",
    "\n",
    "    data = {\n",
    "        \"evidence\": evidence,\n",
    "        \"global_chi2/bins\": global_chi2_bins,\n",
    "        \"model_string\": str(model),\n",
    "    }\n",
    "\n",
    "    if window_func:\n",
    "        mask = regression.getBlindedMask(\n",
    "            pred_data.X, pred_data.Y, test_data.Y, test_data.V, window_func\n",
    "        )\n",
    "        bpred_mean = pred_data.Y[mask]\n",
    "        obs_mean = test_data.Y[mask]\n",
    "        obs_var = test_data.V[mask]\n",
    "        chi2 = torch.sum((obs_mean - bpred_mean) ** 2 / obs_var) / torch.count_nonzero(\n",
    "            mask\n",
    "        )\n",
    "        avg_pull = torch.sum(\n",
    "            torch.abs((obs_mean - bpred_mean)) / torch.sqrt(obs_var)\n",
    "        ) / torch.count_nonzero(mask)\n",
    "\n",
    "        data.update(\n",
    "            {\n",
    "                \"chi2_blinded\": float(chi2),\n",
    "                \"avg_abs_pull\": float(avg_pull),\n",
    "            }\n",
    "        )\n",
    "        print(f\"Avg Abs pull = {avg_pull}\")\n",
    "        print(f\"Chi^2/bins = {chi2}\")\n",
    "    else:\n",
    "        mask = None\n",
    "    print(f\"Global Chi2/bins = {global_chi2_bins}\")\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if True:\n",
    "        diagnostic_plots = makeDiagnosticPlots(pred_data, test_data, train_data, mask)\n",
    "        saveDiagnosticPlots(diagnostic_plots, save_dir)\n",
    "        # makeSlicePlots(pred_data, test_data, inhist, window_func, 0, save_dir)\n",
    "        # makeSlicePlots(pred_data, test_data, inhist, window_func, 1, save_dir)\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    save_data = RegressionModel(\n",
    "        input_data=inhist,\n",
    "        window=window_func,\n",
    "        domain_mask=domain_mask,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        trained_model=model,\n",
    "        raw_posterior_dist=raw_pred_dist,\n",
    "        posterior_dist=pred_dist,\n",
    "    )\n",
    "    torch.save(save_data, save_dir / \"train_model.pth\")\n",
    "    torch.save(save_data, save_dir / \"model_dict.pth\")\n",
    "    torch.save(pred_dist, save_dir / \"posterior_latent.pth\")\n",
    "    with open(save_dir / \"info.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ca4a7-4145-4645-bbad-728bd596a163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
